main:
  params: [args]
  steps:
    - init:
        assign:
        - project: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
        - location: ${default(map.get(args, "location"), "us-central1")}
        - cloud_function: ${default(map.get(args, "cloud_function"), "gaarf")}
        - bq_dataset_location: ${default(map.get(args, "bq_dataset_location"), "us")}
        - gcs_bucket: ${default(map.get(args,"gcs_bucket"), project)}
    - run_ads_workflow:
        call: runAdsQueries
        args:
          project: ${project}
          location: ${location}
          function_name: ${cloud_function}
          gcs_bucket: ${gcs_bucket}
          queries_path: ${args.ads_queries_path}
          bq_dataset: ${args.dataset}
          bq_dataset_location: ${bq_dataset_location}
          cid: ${args.cid}
          macros: ${map.get(args, "ads_macro")}
          ads_config_path: ${args.ads_config_path}
    - run_bq_workflow:
        call: runBigQueryQueries
        args:
          project: ${project}
          location: ${location}
          function_name: ${cloud_function + "-bq"}
          gcs_bucket: ${gcs_bucket}
          queries_path: ${args.bq_queries_path}
          dataset_location: ${bq_dataset_location}
          macros: ${map.get(args, "bq_macro")}
          sqlParams: ${map.get(args, "bq_sql")}

runAdsQueries:
  params: [project, location, function_name, gcs_bucket, queries_path, bq_dataset, bq_dataset_location, cid, macros, ads_config_path]
  # NOTE: currently it's assumed that CF's project is the same as project for BQ datasets
  steps:
    # get CF function URL (specially usefull for v2 CF)
    - get_function:
        call: http.get
        args:
          url: ${"https://cloudfunctions.googleapis.com/v2/projects/" + project + "/locations/" + location + "/functions/" + function_name}
          auth:
            type: OAuth2
        result: function
        # TODO: move to using CF adapter when it's ready (currently only v1 supported):
        #call: googleapis.cloudfunctions.v2.projects.locations.functions.get
        #args:
        #  name: ${"projects/" + project + "/locations/" + location + "/functions/" + function_name}
        #result: function
    - log_functions_metadata:
        call: sys.log
        args:
          data: ${function}
    # fetch script from GCS
    - get_ads_scripts_from_gcs:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${gcs_bucket}
          prefix: ${queries_path}
        result: scripts
    - log_ads_scripts:
        call: sys.log
        args:
          data: ${scripts.items}
          severity: "INFO"
    - runAdsQueries:
        parallel:
          shared: [scripts]
          for:
            value: script_item
            in: ${scripts.items}
            steps:
              - call_gaarf_cf:
                  try:
                    call: http.post
                    args:
                      url: ${function.body.serviceConfig.uri}
                      timeout: 1800
                      query:
                        script_path: ${"gs://" + gcs_bucket + "/" + script_item.name}
                        bq_project_id: ${project}
                        bq_dataset: ${bq_dataset}
                        bq_dataset_location: ${bq_dataset_location}
                        customer_id: ${cid}
                        ads_config_path: ${ads_config_path}
                      body:
                        macro: ${macros}
                      auth:
                        type: OIDC
                    result: script_results
                  retry:
                    predicate: ${custom_retry_predicate}
                    max_retries: 3
                    backoff:
                      initial_delay: 2
                      max_delay: 60
                      multiplier: 2
              - log_script_result:
                  call: sys.log
                  args:
                    data: ${script_results.body}
                    severity: "INFO"

custom_retry_predicate:
  params: [e]
  steps:
    - what_to_repeat:
        switch:
          - condition: ${e.code == 500}
            return: true
    - otherwise:
        return: false

runBigQueryQueries:
  params: [project, location, function_name, gcs_bucket, queries_path, macros, sqlParams, dataset_location]

  steps:
    - get_bq_scripts_from_gcs:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${gcs_bucket}
          prefix: ${queries_path}
        result: bq_scripts
    # check if there are any bq scripts on GCS
    - check_scripts:
        switch:
          - condition: ${map.get(bq_scripts, "items") != null and len(map.get(bq_scripts, "items")) > 0}
            next: log_bq_scripts
        next: end
    - log_bq_scripts:
        call: sys.log
        args:
          data: ${bq_scripts.items}
          severity: "INFO"
    # get clound function's uri
    - get_function_bq:
        call: http.get
        args:
          url: ${"https://cloudfunctions.googleapis.com/v2/projects/" + project + "/locations/" + location + "/functions/" + function_name}
          auth:
            type: OAuth2
        result: function_bq
        # TODO: move to using CF adapter when it's ready (currently only v1 supported):
        #call: googleapis.cloudfunctions.v2.projects.locations.functions.get
        #args:
        #  name: ${"projects/" + project + "/locations/" + location + "/functions/" + function_name}
        #result: function_bq
    - runBqQueries:
        for:
          value: bq_script_item
          in: ${bq_scripts.items}
          steps:
            - call_gaarf_bq_cf:
                call: http.post
                args:
                  url: ${function_bq.body.serviceConfig.uri}
                  query:
                    script_path: ${"gs://" + gcs_bucket + "/" + bq_script_item.name}
                    project_id: ${project}
                    dataset_location: ${dataset_location}
                  body:
                    macro: ${macros}
                    sql: ${sqlParams}
                  auth:
                    type: OIDC
                result: script_results
            - log_script_bq_result:
                call: sys.log
                args:
                  data: ${script_results.body}
                  severity: "INFO"
# TODO:
# * safely ignore the absence of scripts on GCS
# * BQ scripts should be executed sequentially
