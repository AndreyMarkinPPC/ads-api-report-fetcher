main:
  params: [args]
  steps:
    - init:
        assign:
        - project: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
        - location: ${default(map.get(args, "location"), "us-central1")}
        - cloud_function: ${default(map.get(args, "cloud_function"), "gaarf")}
        - bq_dataset_location: ${default(map.get(args, "bq_dataset_location"), "us")}
        - gcs_bucket: ${default(map.get(args,"gcs_bucket"), project)}
    - run_ads_workflow:
        call: runAdsQueries
        args:
          project: ${project}
          location: ${location}
          function_name: ${cloud_function}
          gcs_bucket: ${gcs_bucket}
          queries_path: ${args.ads_queries_path}
          bq_dataset: ${args.dataset}
          bq_dataset_location: ${bq_dataset_location}
          cid: ${args.cid}
          macros: ${map.get(args, "ads_macro")}
          ads_config_path: ${args.ads_config_path}
    - run_bq_workflow:
        call: runBigQueryQueries
        args:
          project: ${project}
          location: ${location}
          function_name: ${cloud_function + "-bq"}
          gcs_bucket: ${gcs_bucket}
          queries_path: ${args.bq_queries_path}
          dataset_location: ${bq_dataset_location}
          macros: ${map.get(args, "bq_macro")}
          sqlParams: ${map.get(args, "bq_sql")}

runAdsQueries:
  params: [project, location, function_name, gcs_bucket, queries_path, bq_dataset, bq_dataset_location, cid, macros, ads_config_path]
  # NOTE: currently it's assumed that CF's project is the same as project for BQ datasets
  steps:
    # get CF function URL (specially usefull for v2 CF)
    - get_function:
        call: googleapis.cloudfunctions.v1.projects.locations.functions.get
        args:
          name: ${"projects/" + project + "/locations/" + location + "/functions/" + function_name}
        result: function
    - log_functions_metadata:
        call: sys.log
        args:
          data: ${function}
    # fetch script from GCS
    - get_ads_scripts_from_gcs:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${gcs_bucket}
          prefix: ${queries_path}
        result: scripts
    - log_ads_scripts:
        call: sys.log
        args:
          data: ${scripts.items}
          severity: "INFO"
    - runAdsQueries:
        parallel:
          shared: [scripts]
          for:
            value: script_item
            in: ${scripts.items}
            steps:
              - call_gaarf_cf:
                  call: http.post
                  args:
                    url: ${function.httpsTrigger.url}
                    query:
                      script_path: ${"gs://" + gcs_bucket + "/" + script_item.name}
                      bq_project_id: ${project}
                      bq_dataset: ${bq_dataset}
                      bq_dataset_location: ${bq_dataset_location}
                      customer_id: ${cid}
                      ads_config_path: ${ads_config_path}
                    body:
                      macro: ${macros}
                    auth:
                      type: OIDC
                      audience: ${function.httpsTrigger.url}
                  result: script_results
              - log_script_result:
                  call: sys.log
                  args:
                    data: ${script_results.body}
                    severity: "INFO"

runBigQueryQueries:
  params: [project, location, function_name, gcs_bucket, queries_path, macros, sqlParams, dataset_location]

  steps:
    - get_bq_scripts_from_gcs:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${gcs_bucket}
          prefix: ${queries_path}
        result: bq_scripts
    # check if there are any bq scripts on GCS
    - check_scripts:
        switch:
          - condition: ${map.get(bq_scripts, "items") != null and len(map.get(bq_scripts, "items")) > 0}
            next: get_function_bq
        next: end
    - log_bq_scripts:
        call: sys.log
        args:
          data: ${bq_scripts.items}
          severity: "INFO"
    # get clound function's uri
    - get_function_bq:
        call: googleapis.cloudfunctions.v1.projects.locations.functions.get
        args:
          name: ${"projects/" + project + "/locations/" + location + "/functions/" + function_name}
        result: function_bq
    - runBqQueries:
        for:
          value: bq_script_item
          in: ${bq_scripts.items}
          steps:
            - call_gaarf_bq_cf:
                call: http.post
                args:
                  url: ${function_bq.httpsTrigger.url}
                  query:
                    script_path: ${"gs://" + gcs_bucket + "/" + bq_script_item.name}
                    project_id: ${project}
                    dataset_location: ${dataset_location}
                  body:
                    macro: ${macros}
                    sql: ${sqlParams}
                  auth:
                    type: OIDC
                    audience: ${function_bq.httpsTrigger.url}
                result: script_results
            - log_script_bq_result:
                call: sys.log
                args:
                  data: ${script_results.body}
                  severity: "INFO"
# TODO:
# * safely ignore the absence of scripts on GCS
# * BQ scripts should be executed sequentially
